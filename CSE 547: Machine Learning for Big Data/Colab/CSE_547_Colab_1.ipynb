{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# CSE547 - Colab 1\n",
        "## Wordcount in Spark\n",
        "\n",
        "Adapted From Stanford CS246"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d61412-401b-4d87-8f4e-7f1b23a2d310"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/autocompletion.py\", line 12, in <module>\n",
            "    from pip._internal.metadata import get_default_environment\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/metadata/__init__.py\", line 3, in <module>\n",
            "    from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/metadata/base.py\", line 21, in <module>\n",
            "    from pip._vendor.packaging.requirements import Requirement\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/packaging/requirements.py\", line 10, in <module>\n",
            "    from pip._vendor.pyparsing import (  # noqa\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/__init__.py\", line 141, in <module>\n",
            "    from .helpers import *\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/helpers.py\", line 688, in <module>\n",
            "    common_html_entity = Regex(\"&(?P<entity>\" + \"|\".join(_htmlEntityMap) + \");\").set_name(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 2933, in __init__\n",
            "    self.re = re.compile(self.pattern, self.flags)\n",
            "  File \"/usr/lib/python3.8/re.py\", line 252, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.8/re.py\", line 304, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.8/sre_compile.py\", line 764, in compile\n",
            "    p = sre_parse.parse(p, flags)\n",
            "  File \"/usr/lib/python3.8/sre_parse.py\", line 948, in parse\n",
            "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
            "  File \"/usr/lib/python3.8/sre_parse.py\", line 443, in _parse_sub\n",
            "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
            "  File \"/usr/lib/python3.8/sre_parse.py\", line 834, in _parse\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "  File \"/usr/lib/python3.8/sre_parse.py\", line 443, in _parse_sub\n",
            "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
            "  File \"/usr/lib/python3.8/sre_parse.py\", line 529, in _parse\n",
            "    subpatternappend((LITERAL, _ord(this)))\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "openjdk-8-jdk-headless is already the newest version (8u352-ga-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orRvrc1-545"
      },
      "source": [
        "id='1SE6k_0YukzGd5wK-E4i6mG83nydlfvSa'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('pg100.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "If you executed the cells above, you should be able to see the file *pg100.txt* under the \"Files\" tab on the left panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "If you run successfully the setup stage, you are ready to work on the *pg100.txt* file which contains a copy of the complete works of Shakespeare.\n",
        "\n",
        "Write a Spark application which outputs the number of words that start with each letter. This means that for every letter we want to count the total number of (non-unique) words that start with a specific letter. In your implementation **ignore the letter case**, i.e., consider all words as lower case. Also, you can ignore all the words **starting** with a non-alphabetic character.\n",
        "\n",
        "For this task we ask you to the [**RDD MapReduce API**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html) from spark (map, reduceByKey, flatMap, etc.) instead of **DataFrame API**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-e7Ph2_ruG"
      },
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuAxGFPFB43Y"
      },
      "source": [
        "f = sc.textFile(\"/content/pg100.txt\")\n",
        "tokens= f.flatMap(lambda line : line.strip().lower().split(\" \"))\n",
        "words = tokens.map(lambda w: re.sub(r'[^a-z]', 'z', w)).filter(lambda w: len(w) > 0)\n",
        "word_counts= words.map(lambda word: (list(word)[0],1)).reduceByKey(lambda a,b:a+b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word_counts= tokens.map(lambda word: (list(word)[0],1)).reduceByKey(lambda a,b:a+b)"
      ],
      "metadata": {
        "id": "SgScHdzb7QGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_word_counts = word_counts.sortBy(lambda key : -key[1]).collect()"
      ],
      "metadata": {
        "id": "XA7u1vW-OHsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHETdIzF7QIZ",
        "outputId": "2673adeb-4d82-4126-a009-b60dfcd0ad13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t', 123602),\n",
              " ('a', 84836),\n",
              " ('s', 65705),\n",
              " ('i', 62167),\n",
              " ('h', 60563),\n",
              " ('w', 59597),\n",
              " ('m', 55676),\n",
              " ('b', 45455),\n",
              " ('o', 43494),\n",
              " ('f', 36814),\n",
              " ('c', 34567),\n",
              " ('d', 29713),\n",
              " ('l', 29569),\n",
              " ('p', 27759),\n",
              " ('n', 26759),\n",
              " ('y', 25855),\n",
              " ('g', 20782),\n",
              " ('e', 18697),\n",
              " ('r', 14265),\n",
              " ('k', 9418),\n",
              " ('u', 9170),\n",
              " ('z', 8140),\n",
              " ('v', 5728),\n",
              " ('j', 3339),\n",
              " ('q', 2377),\n",
              " ('x', 14)]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTAFPGFVLvHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVHVgGYnNz8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ObTdzj5CN0Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0\n",
        "\n",
        "for i in sorted_word_counts:\n",
        "  if i[0] and i[0][0] == \"t\":\n",
        "    res += i[1]\n",
        "\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWgtU2Zm7QM1",
        "outputId": "cf751df1-19db-4539-d955-e2df8d8884b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123602"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0\n",
        "\n",
        "for i in sorted_word_counts:\n",
        "  if i[0] and i[0][0] == \"i\":\n",
        "    res += i[1]\n",
        "\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNo9j6pe_Zc5",
        "outputId": "ad5aea6f-f219-4b49-fc77-fa4f93e99ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62167"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0\n",
        "\n",
        "for i in sorted_word_counts:\n",
        "  if i[0] and i[0][0] == \"m\":\n",
        "    res += i[1]\n",
        "\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT2YE2Wz_0Sw",
        "outputId": "c9d1d135-7eb5-4a39-ff78-e131222d21c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55676"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Titles.registerTempTable(\"Titles\")\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT titleType, count(*) as numTitles\n",
        "FROM Titles\n",
        "WHERE \n",
        "\"\"\"\n",
        "\n",
        "title_type_counts = spark.sql(query)\n",
        "title_type_counts.show()"
      ],
      "metadata": {
        "id": "QN_L6KBu8rTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrXJyVNP2AI"
      },
      "source": [
        "Once you obtained the desired results, **head over to Gradescope and submit your solution for this Colab**!"
      ]
    }
  ]
}