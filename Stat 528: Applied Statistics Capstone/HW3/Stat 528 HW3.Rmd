---
title: "Stat 528 HW3"
author: "Dongyang Wang"
date: "2023-02-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1


## Question 2

### Question 2.1

```{r}
rm(list = ls())
psycho.file<-"psycho.txt"
n<-max(count.fields(psycho.file))
psycho.mat<-data.matrix(read.table(psycho.file,fill=TRUE,col.names=1:n))
colnames(psycho.mat)<-c(paste0("Performance_",c(1:8)),
paste0("Written_",c("Verbal","Spatial","Arithmetic")),
paste0("Practical_",c("Operating","ReactionTime","RateofMovement")))
rownames(psycho.mat)<-colnames(psycho.mat)
```

```{r}
library(psych)

psycho.mat1 = psycho.mat


for (i in 1: 14){
  for (j in 1:14){
    if (i<j){
      psycho.mat1[i,j] = psycho.mat1[j,i]
    }
    else{
      psycho.mat1[i,j] = psycho.mat1[i,j]
    }
  }
}

# Calculate the correlation matrix first
bfi_EFA_cor <- cor(psycho.mat1, use = "everything")

# Then use that correlation matrix to create the scree plot
scree(bfi_EFA_cor, factors = FALSE)

# The following code of factanal will work
#factanal(covmat = psycho.mat1, factors = 2, rotation = "none")
```

### Question 2.2

Disregard, factanal won't run.

```{r}
psycho.mat2 = as.data.frame(psycho.mat1)
# Calculate the correlation matrix first
bfi_EFA_cor1 <- cor(psycho.mat2)

# Then use that correlation matrix to create the scree plot
scree(bfi_EFA_cor1, factors = FALSE)

factanal(psycho.mat2, factors =2, scores = "regression")
```

```{r}
#oblimin_mod <- fa(psycho.mat1, nfactors = 2, rotate = "oblimin")
#scree(, factors = FALSE)
#oblimin_mod
```
Use the following.

```{r}
no_rotation_mod <- fa(psycho.mat1, nfactors = 2, rotate = "none")
print(no_rotation_mod$loadings,cutoff=0)

norotate_df = as.data.frame(matrix(c("Performance_1"   ,          0.744 , 0.198,
"Performance_2"           ,  0.905 , 0.005,
"Performance_3"          ,   0.906 ,-0.019,
"Performance_4"           ,  0.933, -0.138,
"Performance_5"           ,  0.931 ,-0.183,
"Performance_6"           ,  0.894 ,-0.212,
"Performance_7"           ,  0.901 ,-0.185,
"Performance_8"           ,  0.887 ,-0.196,
"Written_Verbal"          ,  0.344 , 0.486,
"Written_Spatial"         ,  0.541 , 0.766,
"Written_Arithmetic"      ,  0.444 , 0.418,
"Practical_Operating"     ,  0.461 ,-0.030,
"Practical_ReactionTime"   , 0.287 ,-0.183,
"Practical_RateofMovement" , 0.363 ,-0.019), nrow = 14, byrow =T))
colnames(norotate_df) = c("Variable", "Principal_Comp_1","Principal_Comp_2" )
norotate_df$Principal_Comp_1 = as.numeric(norotate_df$Principal_Comp_1)
norotate_df$Principal_Comp_2 = as.numeric(norotate_df$Principal_Comp_2)

#install.packages("devtools")
#devtools::install_github("slowkow/ggrepel")
library(ggrepel)

ggplot(norotate_df, aes(x = Principal_Comp_1, y = Principal_Comp_2, label = Variable)) +
  geom_point()+
  geom_text_repel() +
  ggtitle("No Rotation 2 Factor Model")

```



### Question 2.3

```{r}
library(GPArotation)
oblimin_mod <- fa(psycho.mat1, nfactors = 2, rotate = "oblimin")
print(oblimin_mod$loadings,cutoff=0)

oblimin_df = as.data.frame(matrix(c("Performance_1"   ,          0.548,  0.349,
"Performance_2"           ,  0.819 , 0.163,
"Performance_3"          ,   0.836,  0.137,
"Performance_4"           ,  0.938,  0.010,
"Performance_5"           ,  0.966, -0.040,
"Performance_6"           ,  0.951, -0.079,
"Performance_7"           ,  0.941, -0.048,
"Performance_8"           ,  0.935, -0.062,
"Written_Verbal"          , -0.004,  0.598,
"Written_Spatial"         ,  -0.008,  0.942,
"Written_Arithmetic"      ,  0.131 , 0.540,
"Practical_Operating"     ,  0.439,  0.047,
"Practical_ReactionTime"   , 0.380, -0.152,
"Practical_RateofMovement" , 0.342,  0.042), nrow = 14, byrow =T))

colnames(oblimin_df) = c("Variable", "Principal_Comp_1","Principal_Comp_2" )
oblimin_df$Principal_Comp_1 = as.numeric(oblimin_df$Principal_Comp_1)
oblimin_df$Principal_Comp_2 = as.numeric(oblimin_df$Principal_Comp_2)

#install.packages("devtools")
#devtools::install_github("slowkow/ggrepel")
library(ggrepel)

ggplot(oblimin_df, aes(x = Principal_Comp_1, y = Principal_Comp_2, label = Variable)) +
  geom_point()+
  geom_text_repel() +
  ggtitle("Oblimin Rotation 2 Factor Model")

```


### Question 3.1.1

```{r}
library(dslabs)
mnist <- read_mnist(
  path = NULL,
  download = FALSE,
  destdir = tempdir(),
  url = "https://www2.harvardx.harvard.edu/courses/IDS_08_v2_03/",
  keep.files = TRUE
)

mnist_data <- mnist$train$images

#summary(mnist_data[,100:150])
```

```{r}
pca = princomp(mnist_data, cor = F)
names(pca)
```

### Question 3.1.2

```{r}
#calculate total variance explained by each principal component
var_explained = pca$sdev^2 / sum(pca$sdev^2)

#create scree plot
library(ggplot2)

qplot(c(1:10), var_explained[1:10]) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot for MNIST Data") 
```

### Question 3.1.3

```{r}
library(xtable)
res = c()
for (i in 1:7){
  cur = c(var_explained[i], sum(var_explained[1:i]))
  res = rbind(res, cur)
}
colnames(res) = c("Variance Explained", "Cumulative Variance Explained")
rownames(res) = c(paste0("Principal_Component_",c(1:7)))
xtable(res)
```

### Question 3.1.4

```{r}
#library(devtools)
#install_github("vqv/ggbiplot")
#library(ggbiplot)
#ggbiplot(pca, obs.scale = 1, var.scale = 1)

# project data onto the PCA space
#scale(mnist_data, pca$center, pca$scale)
biplot(pca)
```

### Question 3.2.1

```{r}
library(keras)
library(tensorflow)
use_condaenv("r-reticulate")
#use_condaenv("keras-tf", required = T)
K <- keras::backend()
#install.packages("keras")
#install_keras(Tensorflow = "1.13.1",restart_session = FALSE)
#install.packages("remotes")
#remotes::install_github("rstudio/tensorflow")
#library(tensorflow)
#install_tensorflow(version = "2.6.2",method = 'conda', envname = 'r-reticulate')
```

The GitHub code is as follows. Did not use, too abstruse.

```{r, include = F}
# Parameters --------------------------------------------------------------

batch_size <- 100L
original_dim <- 784L
latent_dim <- 2L
intermediate_dim <- 256L
epochs <- 50L
epsilon_std <- 1.0

# Model definition --------------------------------------------------------

x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)

sampling <- function(arg){
  z_mean <- arg[, 1:(latent_dim)]
  z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
  
  epsilon <- k_random_normal(
    shape = c(k_shape(z_mean)[[1]]), 
    mean=0.,
    stddev=epsilon_std
  )
  
  z_mean + k_exp(z_log_var/2)*epsilon
}


# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>% 
  layer_lambda(sampling)

# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)

# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)

# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)

# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)


vae_loss <- function(x, x_decoded_mean){
  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = "rmsprop", loss = vae_loss)

# Data preparation --------------------------------------------------------

x_train <- mnist$train$images/255
x_test <- mnist$test$images/255
x_train <- array_reshape(x_train, c(nrow(x_train), 784), order = "F")
x_test <- array_reshape(x_test, c(nrow(x_test), 784), order = "F")


# Model training ----------------------------------------------------------

vae %>% fit(
  x_train, x_train, 
  shuffle = TRUE, 
  epochs = epochs, 
  batch_size = batch_size, 
  validation_data = list(x_test, x_test)
)


# Visualizations ----------------------------------------------------------

library(ggplot2)
library(dplyr)
x_test_encoded <- predict(encoder, x_test, batch_size = batch_size)

x_test_encoded %>%
  as_data_frame() %>% 
  mutate(class = as.factor(mnist$test$y)) %>%
  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()

# display a 2D manifold of the digits
n <- 15  # figure with 15x15 digits
digit_size <- 28

# we will sample n points within [-4, 4] standard deviations
grid_x <- seq(-4, 4, length.out = n)
grid_y <- seq(-4, 4, length.out = n)

rows <- NULL
for(i in 1:length(grid_x)){
  column <- NULL
  for(j in 1:length(grid_y)){
    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)
    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = 28) )
  }
  rows <- cbind(rows, column)
}
rows %>% as.raster() %>% plot()
```

see https://www.datatechnotes.com/2020/06/how-to-build-variational-autoencoders-in-R.html

```{r}
c(c(xtrain, ytrain), c(xtest, ytest)) %<-% dataset_mnist()
print(dim(xtrain))

input_size = dim(xtrain)[2]*dim(xtrain)[3]
x_train = xtrain/255
x_test = xtest/255
latent_size = 2

x_train <- array_reshape(x_train, c(nrow(x_train), input_size))
x_test <- array_reshape(x_test, c(nrow(x_test), input_size))
print(dim(x_train))

enc_input <- layer_input(shape = c(input_size))

layer_one <- layer_dense(enc_input, units=256, activation = "relu")
z_mean <- layer_dense(layer_one, latent_size)
encoder <- keras_model(enc_input, z_mean)

z_log_var <- layer_dense(layer_one, latent_size)
 
encoder <- keras_model(enc_input, z_mean)
summary(encoder)

sampling <- function(arg){
  z_mean <- arg[, 1:(latent_size)]
  z_log_var <- arg[, (latent_size + 1):(2 * latent_size)]
  epsilon <- k_random_normal(shape = c(k_shape(z_mean)[[1]]), mean=0)
  z_mean + k_exp(z_log_var/2)*epsilon
}

z <- layer_concatenate(list(z_mean, z_log_var)) %>% 
  layer_lambda(sampling)

decoder_layer <- layer_dense(units = 256, activation = "relu")
decoder_mean <- layer_dense(units = input_size, activation = "sigmoid")
h_decoded <- decoder_layer(z)
x_decoded_mean <- decoder_mean(h_decoded)
 
vae <- keras_model(enc_input, x_decoded_mean)
summary(vae)

vae_loss <- function(input, x_decoded_mean){
  xent_loss=(input_size/1.0)*loss_binary_crossentropy(input, x_decoded_mean)
  kl_loss=-0.5*k_mean(1+z_log_var-k_square(z_mean)-k_exp(z_log_var), axis=-1)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = "rmsprop", loss = vae_loss)

dec_input <- layer_input(shape = latent_size)
h_decoded_2 <- decoder_layer(dec_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(dec_input, x_decoded_mean_2)
summary(generator)


tensorflow::tf$compat$v1$disable_eager_execution()

vae %>% fit(
  x_train, x_train, 
  shuffle = TRUE, 
  epochs = 10, 
  batch_size = 64, 
  validation_data = list(x_test, x_test)
)

```

```{r}
n = 3
test =  x_train[0:n,]
x_test_encoded <- predict(encoder, test)
 
decoded_imgs = generator %>% predict(x_test_encoded)
pred_images = array_reshape(decoded_imgs, dim=c(dim(decoded_imgs)[1], 28, 28))
orig_imgaes = array_reshape(test, dim=c(dim(test)[1], 28, 28))

op = par(mfrow=c(n,2), mar=c(1,0,0,0))
for (i in 1:n) 
{
  plot(as.raster(orig_imgaes[i,,]))
  plot(as.raster(pred_images[i,,]))
}
```

```{r}
latent_size = 3

z_mean <- layer_dense(layer_one, latent_size)
encoder <- keras_model(enc_input, z_mean)

z_log_var <- layer_dense(layer_one, latent_size)
 
encoder <- keras_model(enc_input, z_mean)
summary(encoder)

sampling <- function(arg){
  z_mean <- arg[, 1:(latent_size)]
  z_log_var <- arg[, (latent_size + 1):(2 * latent_size)]
  epsilon <- k_random_normal(shape = c(k_shape(z_mean)[[1]]), mean=0)
  z_mean + k_exp(z_log_var/2)*epsilon
}

z <- layer_concatenate(list(z_mean, z_log_var)) %>% 
  layer_lambda(sampling)

decoder_layer <- layer_dense(units = 256, activation = "relu")
decoder_mean <- layer_dense(units = input_size, activation = "sigmoid")
h_decoded <- decoder_layer(z)
x_decoded_mean <- decoder_mean(h_decoded)
 
vae <- keras_model(enc_input, x_decoded_mean)
summary(vae)

vae_loss <- function(input, x_decoded_mean){
  xent_loss=(input_size/1.0)*loss_binary_crossentropy(input, x_decoded_mean)
  kl_loss=-0.5*k_mean(1+z_log_var-k_square(z_mean)-k_exp(z_log_var), axis=-1)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = "rmsprop", loss = vae_loss)

dec_input <- layer_input(shape = latent_size)
h_decoded_2 <- decoder_layer(dec_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(dec_input, x_decoded_mean_2)
summary(generator)


tensorflow::tf$compat$v1$disable_eager_execution()

vae %>% fit(
  x_train, x_train, 
  shuffle = TRUE, 
  epochs = 10, 
  batch_size = 64, 
  validation_data = list(x_test, x_test)
)


```

```{r}
n = 3
test =  x_train[0:n,]
x_test_encoded <- predict(encoder, test)
 
decoded_imgs = generator %>% predict(x_test_encoded)
pred_images = array_reshape(decoded_imgs, dim=c(dim(decoded_imgs)[1], 28, 28))
orig_imgaes = array_reshape(test, dim=c(dim(test)[1], 28, 28))

op = par(mfrow=c(n,2), mar=c(1,0,0,0))
for (i in 1:n) 
{
  plot(as.raster(orig_imgaes[i,,]))
  plot(as.raster(pred_images[i,,]))
}
```

```{r}
latent_size = 4

z_mean <- layer_dense(layer_one, latent_size)
encoder <- keras_model(enc_input, z_mean)

z_log_var <- layer_dense(layer_one, latent_size)
 
encoder <- keras_model(enc_input, z_mean)
summary(encoder)

sampling <- function(arg){
  z_mean <- arg[, 1:(latent_size)]
  z_log_var <- arg[, (latent_size + 1):(2 * latent_size)]
  epsilon <- k_random_normal(shape = c(k_shape(z_mean)[[1]]), mean=0)
  z_mean + k_exp(z_log_var/2)*epsilon
}

z <- layer_concatenate(list(z_mean, z_log_var)) %>% 
  layer_lambda(sampling)

decoder_layer <- layer_dense(units = 256, activation = "relu")
decoder_mean <- layer_dense(units = input_size, activation = "sigmoid")
h_decoded <- decoder_layer(z)
x_decoded_mean <- decoder_mean(h_decoded)
 
vae <- keras_model(enc_input, x_decoded_mean)
summary(vae)

vae_loss <- function(input, x_decoded_mean){
  xent_loss=(input_size/1.0)*loss_binary_crossentropy(input, x_decoded_mean)
  kl_loss=-0.5*k_mean(1+z_log_var-k_square(z_mean)-k_exp(z_log_var), axis=-1)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = "rmsprop", loss = vae_loss)

dec_input <- layer_input(shape = latent_size)
h_decoded_2 <- decoder_layer(dec_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(dec_input, x_decoded_mean_2)
summary(generator)


tensorflow::tf$compat$v1$disable_eager_execution()

vae %>% fit(
  x_train, x_train, 
  shuffle = TRUE, 
  epochs = 10, 
  batch_size = 64, 
  validation_data = list(x_test, x_test)
)


```

```{r}
n = 3
test =  x_train[0:n,]
x_test_encoded <- predict(encoder, test)
 
decoded_imgs = generator %>% predict(x_test_encoded)
pred_images = array_reshape(decoded_imgs, dim=c(dim(decoded_imgs)[1], 28, 28))
orig_imgaes = array_reshape(test, dim=c(dim(test)[1], 28, 28))

op = par(mfrow=c(n,2), mar=c(1,0,0,0))
for (i in 1:n) 
{
  plot(as.raster(orig_imgaes[i,,]))
  plot(as.raster(pred_images[i,,]))
}
```

### Question 3.2.2

```{r}

```


```{r}

```


```{r}


```


### Question 3.2.3



### Question 3

```{r}

```
