---
title: "Stat 504 HW 4"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Q1

### a

$E(Y_i|X_i) = \alpha +\beta X_i$. It is linear on $X_i$ as we can observe.

### b

$Var(Y_i|X_i) = \sigma^2 +\lambda X_i^2$. It is not constant on $X_i$ since changes in $X_i$ lead to changes in the variance.

### c

```{r}
#rm(list = ls())
library(sandwich)
library(estimatr)
library(sandwich)

set.seed(42)
m = 1000

# write simulation function
sim_fun <- function(n = 100, lambda = 0, s2 = 1, a = 0.5, b = 1){
    
    # simulate data
    x   <- rnorm(n,0,1)
    mean <-  a+b*x
    se <-sqrt(s2 + lambda*x^2)
    y   <- rnorm(n,mean, se)
   
    # fit ols
    ols <- lm(y ~ x)
    
    # compute traditional ci
    ci <- confint(ols)
  
    # check if trad ci covers true b1
    trad.in <- ci["x", 1] <= b & b <= ci["x", 2]

    ols1 <- lm_robust(y ~ x)
    # compute robust ci
    #capture.output(
        ## capture.output serves to suppress printing
        #rob.ci <- Confint(ols, vcov. = vcovHC(ols, "HC0"))
    #)
    ###### CANNOT RUN ##########
    #rob.ci <- confint(ols1, vcov. = vcovHC(ols, "HC0"))
    #S(ols, vcov = vcovHC(m, type = "HC0"))
    
    rob.ci <- confint(ols1)
    
    # check if rob ci covers true b1
    rob.in <- rob.ci["x", 1] <= b & b <= rob.ci["x", 2]
    
    # return results
    c(trad = trad.in, rob = rob.in)
}

lambda_n = seq(0,10,1)
sims = expand.grid(lambda_n)


for(i in 1:length(lambda_n)){
    cat("Simulation", i, "of", 10, "\n")
    cat("-params: lambda =", i)
  
    # simulates 1000 times
    sims_i <- replicate(m, sim_fun(n = 100, lambda = i, s2 = 1, a = 0.5, b = 1))
    
    # computes coverage
    sims[i, c(2,3)] <- apply(sims_i, 1, mean)
} 

sims
```

### d

Our results show that as heteroskedasticity occurs, robust standard errors is a solution to this problem. Compared with the traditional standard error which provides a barely good confidence interval, the confidence interval generated by the robust standard error is fairly "robust", meaning that it is correctly covering the true mean for a good amount of time.

## Q2

### a

```{r}
library(haven)
discrimination_df <- read_dta('bm.dta')
summary(lm(call ~ black, data = discrimination_df))
```

The estimated coefficient is -0.032033, meaning that people with black names tend to have a 0.032033 less chance of being called back than people with white sounding names. For white sound names, the rate is 0.096509. For black sound names, the rate is 0.096509 - 0.032033 = 0.064476.

### b

No, we can still run a linear regression and construct a confidence interval for estimation. Because we can use a BLP to explore th correlation between the two variables without constraint.

### c

```{r}
lm_robust(call ~ black, data = discrimination_df)
```

As shown above, the confidence interval using robust standard errors is [-0.04729491, -0.0167708].

### d

```{r}
summary(lm(call ~ black + female + yearsexp, data = discrimination_df))
```

No, it does not change much. Race is potentially a quite important factor (consciously or subconsciously) when hiring people decide whether to hire a person, among all the factors.

### e

If our assumption of unconfoundedness only includes gender ands years of experience, yes we can interpret the result causally. But be careful that a few other factors have not been taken into account, such as education and computer skills in the data set. We need to consider those before making a conclusion. But if everything has been adjusted for, we can claim some causal relationship because the researchers were able to intervene in this experiment and this name sounding psuedo race can is the D variable in our causal story. 

## Q3

### a

```{r}
election_df <- read.csv('hibbs.dat', sep = '', header = T)
library(tidyverse)
election_model <- lm(vote ~ growth, data = election_df)
ggplot(election_df, aes(x = growth, y = vote)) + 
            geom_point() + 
            geom_line(aes(y = fitted(election_model)), col = "red")
```



### b

```{r}
summary(election_model)
confint(election_model)
```

1% increase in average growth is associated with 3.0605 increase in vote share. If no growth, vote share is predicted to be 46.2476.

### c

```{r}
# confidence level
alpha <- 0.05

# classical parametric confidence interval
param.ci <- predict(election_model, interval = "confidence",newdata = election_df, level = 1-alpha)

# robust parametric confidence interval
election_model1 <- lm_robust(vote ~ growth, data = election_df)
robust.ci <- predict(election_model1, interval = "confidence", newdata = election_df, level = 1-alpha)
robust.ci <- data.frame(robust.ci)
colnames(robust.ci) <- c("robust.fit", "robust.lwr", "robust.upr")

# nonparametric bootstrap confidence interval

# bootstrap function
boot.fun <- function(){
    idx <- sample(nrow(election_df), replace = T)
    ols.boot <- lm(vote ~ growth, data = election_df[idx,])
    yhat.boot <- predict(ols.boot, newdata = election_df)
    return(yhat.boot)
}

# replicate 10,000 times
boot.out <- replicate(10000, boot.fun())

# quantile confidence interval
boot.ci <- t(apply(boot.out, 1, quantile, c(alpha/2, 1-alpha/2)))
colnames(boot.ci) <- c("boot.lwr", "boot.upr")

election <-cbind(election_df, param.ci, robust.ci, boot.ci)

# gpplot
ggplot(election, aes(x = growth, y = vote)) + 
            geom_point() + 
            geom_line(aes(y = fitted(election_model)), col = "red") +
            geom_ribbon(aes(ymin = lwr, ymax = upr, fill = "Parametric", color = "Parametric"), alpha=0.1, linetype="dashed") + 
            geom_ribbon(aes(ymin = boot.lwr, ymax = boot.upr, fill = "Bootstrap", color = "Bootstrap"), alpha=0.1, linetype = "dashed") +
            geom_ribbon(aes(ymin = robust.lwr, ymax = robust.upr, fill = "Robust", color = "Robust"), alpha=0.1, linetype = "dashed") +
            scale_fill_manual(name = "Confidence Interval", values = c("Parametric" = "black", "Bootstrap" = "red", "Robust" = "blue")) +
            scale_color_manual(name = "Confidence Interval", values = c("Parametric" = "black", "Bootstrap" = "red", "Robust" = "blue")) +
            theme_bw()
```
The confidence intervals are listed below

```{r}
election
```

### d

```{r}
# Point estimate using election_model
46.248 + 3.061 * 2
# = 52.37

# confidence level
alpha <- 0.05

election_df1 = election_df['vote']
election_df1$growth <- rep(2,16)

# classical parametric confidence interval
param.ci <- predict(election_model, interval = "confidence",newdata = election_df1, level = 1-alpha)

# robust parametric confidence interval
election_model1 <- lm_robust(vote ~ growth, data = election_df)
robust.ci <- predict(election_model1, interval = "confidence", newdata = election_df1, level = 1-alpha)
robust.ci <- data.frame(robust.ci)
colnames(robust.ci) <- c("robust.fit", "robust.lwr", "robust.upr")

# nonparametric bootstrap confidence interval

# bootstrap function
boot.fun <- function(){
    idx <- sample(nrow(election_df), replace = T)
    ols.boot <- lm(vote ~ growth, data = election_df[idx,])
    yhat.boot <- predict(ols.boot, newdata = election_df1)
    return(yhat.boot)
}

# replicate 10,000 times
boot.out <- replicate(10000, boot.fun())

# quantile confidence interval
boot.ci <- t(apply(boot.out, 1, quantile, c(alpha/2, 1-alpha/2)))
colnames(boot.ci) <- c("boot.lwr", "boot.upr")

election <-cbind(election_df, param.ci, robust.ci, boot.ci)

# gpplot
ggplot(election, aes(x = growth, y = vote)) + 
            geom_point() + 
            geom_line(aes(y = fitted(election_model)), col = "red") +
            geom_ribbon(aes(ymin = lwr, ymax = upr, fill = "Parametric", color = "Parametric"), alpha=0.1, linetype="dashed") + 
            geom_ribbon(aes(ymin = boot.lwr, ymax = boot.upr, fill = "Bootstrap", color = "Bootstrap"), alpha=0.1, linetype = "dashed") +
            geom_ribbon(aes(ymin = robust.lwr, ymax = robust.upr, fill = "Robust", color = "Robust"), alpha=0.1, linetype = "dashed") +
            scale_fill_manual(name = "Confidence Interval", values = c("Parametric" = "black", "Bootstrap" = "red", "Robust" = "blue")) +
            scale_color_manual(name = "Confidence Interval", values = c("Parametric" = "black", "Bootstrap" = "red", "Robust" = "blue")) +
            theme_bw()

```

The point estimate is 52.37. The confidence intervals are listed below

```{r}
election
```

## Q4

### a

```{r}
house_df <- read.csv('SaratogaHouses.csv')
house_model1 <- lm_robust(price ~ fireplaces, data = house_df)
confint(house_model1)
summary(house_model1)
```

A 95% confidence interval is [57437.51,  75960.13]. The existence of fireplaces seems positively correlated with house prices after adjusting for non-constant variance. The houses with fireplaces can be sold 66699 more.

### b

```{r}
house_model2 <- lm_robust(price ~ bedrooms, data = house_df)
confint(house_model2)
summary(house_model2)
```

A 95% confidence interval is [57437.51, 75960.13]. The number of bedrooms seems positively correlated with house prices after adjusting for non-constant variance. The houses with one additional bedroom can be sold 48218 more.

### c

```{r}
house_model3 <- lm_robust(price ~ ., data = house_df)
confint(house_model3)
summary(house_model3)
cor(house_df$room, house_df$bedrooms)
```

For bedroom, the confidence interval is [-1.363845e+04, -2031.932118]; for fireplaces it is [-6.338209e+03, 8411.435710].

Yes, the coefficient of fireplace is no longer significant, and the coefficient of bedroom even becomes negative. The results show that possibly some omitted variable bias has occurred. Fireplaces seem uncorrelated with house prices; the increase in the number of bedrooms is associated with lower house prices. The coefficient of bedrooms has changed possibly due to collinearity because the correlation between room and bedroom is as high as 0.67. Since we already have room with a postive significant coefficient, and if we keep only one of the two correlated variables the coefficient will be normal.

## Q5

### 1
No.

### 2
Yes.