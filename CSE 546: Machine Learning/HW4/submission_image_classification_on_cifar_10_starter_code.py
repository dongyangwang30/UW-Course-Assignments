# -*- coding: utf-8 -*-
"""Submission: image-classification-on-cifar-10-starter-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YRL5VCZPJbpcYI_Uce3V0D8-kUCjQYTB

## Image Classification on CIFAR-10
In this problem we will explore different deep learning architectures for image classification on the CIFAR-10 dataset. Make sure that you are familiar with torch `Tensor`s, two-dimensional convolutions (`nn.Conv2d`) and fully-connected layers (`nn.Linear`), ReLU non-linearities (`F.relu`), pooling (`nn.MaxPool2d`), and tensor reshaping (`view`).

We will use Colab because it has free GPU runtimes available; GPUs can accelerate training times for this problem by 10-100x. **You will need to enable the GPU runtime to use it**. To do so, click "Runtime" above and then "Change runtime type". There under hardware accelerator choose "GPU".

This notebook provides some starter code for the CIFAR-10 problem on HW4, including a completed training loop to assist with some of the Pytorch setup. You'll need to modify this code to implement the layers required for the assignment, but this provides a working training loop to start from.

*Note: GPU runtimes are limited on Colab. Limit your training to short-running jobs (around 20mins or less) and spread training out over time, if possible. Colab WILL limit your usage of GPU time, so plan ahead and be prepared to take breaks during training.* We also suggest performing your early coding/sweeps on a small fraction of the dataset (~10%) to minimize training time and GPU usage.
"""

import torch
from torch import nn
import numpy as np

from typing import Tuple, Union, List, Callable
from torch.optim import SGD
import torchvision
from torch.utils.data import DataLoader, TensorDataset, random_split
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

assert torch.cuda.is_available(), "GPU is not available, check the directions above (or disable this assertion to use CPU)"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)  # this should print out CUDA

train_dataset = torchvision.datasets.CIFAR10("./data", train=True, download=True, transform=torchvision.transforms.ToTensor())
test_dataset = torchvision.datasets.CIFAR10("./data", train=False, download=True, transform=torchvision.transforms.ToTensor())

batch_size = 128

train_dataset, val_dataset = random_split(train_dataset, [int(0.9 * len(train_dataset)), int( 0.1 * len(train_dataset))])

# Create separate dataloaders for the train, test, and validation set
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=True
)

_ , train_small_first = random_split(train_dataset, [int(0.9 * len(train_dataset)), int( 0.1 * len(train_dataset))])
train_small , val_small = random_split(train_small_first, [int(0.9 * len(train_small_first)), int( 0.1 * len(train_small_first))])

train_small_loader = DataLoader(
    train_small,
    batch_size=batch_size,
    shuffle=True
)

val_small_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=True
)

def a_model(M) -> nn.Module:

    model =  nn.Sequential(
            nn.Flatten(),
            nn.Linear(3072, M),
            nn.ReLU(),
            nn.Linear(M, 10)
         )
    return model.to(DEVICE)

"""Let's define a method to train this model using SGD as our optimizer."""

def train(
    model: nn.Module, optimizer: SGD,
    train_loader: DataLoader, val_loader: DataLoader,
    epochs: int = 20
)-> Tuple[List[float], List[float], List[float], List[float]]:
    """
    Trains a model for the specified number of epochs using the loaders.

    Returns:
    Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.
    """

    loss = nn.CrossEntropyLoss()
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    for e in tqdm(range(epochs)):
        model.train()
        train_loss = 0.0
        train_acc = 0.0

        # Main training loop; iterate over train_loader. The loop
        # terminates when the train loader finishes iterating, which is one epoch.
        for (x_batch, labels) in train_loader:
            x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            labels_pred = model(x_batch)
            batch_loss = loss(labels_pred, labels)
            train_loss = train_loss + batch_loss.item()

            labels_pred_max = torch.argmax(labels_pred, 1)
            batch_acc = torch.sum(labels_pred_max == labels)
            train_acc = train_acc + batch_acc.item()

            batch_loss.backward()
            optimizer.step()
        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(train_acc / (batch_size * len(train_loader)))

        # Validation loop; use .no_grad() context manager to save memory.
        model.eval()
        val_loss = 0.0
        val_acc = 0.0

        with torch.no_grad():
            for (v_batch, labels) in val_loader:
                v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)
                labels_pred = model(v_batch)
                v_batch_loss = loss(labels_pred, labels)
                val_loss = val_loss + v_batch_loss.item()

                v_pred_max = torch.argmax(labels_pred, 1)
                batch_acc = torch.sum(v_pred_max == labels)
                val_acc = val_acc + batch_acc.item()
            val_losses.append(val_loss / len(val_loader))
            val_accuracies.append(val_acc / (batch_size * len(val_loader)))

    return train_losses, train_accuracies, val_losses, val_accuracies

def parameter_search_a(
    train_loader: DataLoader,
    val_loader: DataLoader,
    model_fn:Callable[[], nn.Module]
) -> float:
    """
    Parameter search for our linear model using SGD.

    Args:
    train_loader: the train dataloader.
    val_loader: the validation dataloader.
    model_fn: a function that, when called, returns a torch.nn.Module.

    Returns:
    The learning rate with the least validation loss.
    NOTE: you may need to modify this function to search over and return
     other parameters beyond learning rate.
    """
    num_iter = 10  # This will likely not be enough for the rest of the problem.
    best_loss = torch.tensor(np.inf)
    best_lr = 0.0
    best_M = 0.0

    Ms = torch.linspace(100, 900, num_iter).int()
    lrs = torch.linspace(10 ** (-6), 10 ** (-1), num_iter)
    hyperparameter_list = []
    #print(Ms, lrs, type(Ms))
    for M in Ms:
        for lr in lrs:
          print(f"trying learning rate {lr}", f"M value is {M}")
          model = a_model(M)
          optim = SGD(model.parameters(), lr)

          train_loss, train_acc, val_loss, val_acc = train(
              model,
              optim,
              train_loader,
              val_loader,
              epochs=5
              )

          if min(val_loss) < best_loss:
              best_loss = min(val_loss)
              best_lr = lr
              best_M = M
              #print(type(best_lr))
              hyperparameter_list.append((best_loss, best_lr, best_M))

    return best_lr, best_M, hyperparameter_list

"""Now that we have everything, we can train and evaluate our model."""

best_lr, best_M, hyperparameter_list = parameter_search_a(train_small_loader, val_small_loader, a_model)

best_lr.item(), best_M, hyperparameter_list

num_iter = 10
Ms = torch.linspace(100, 900, num_iter).int()
lrs = torch.linspace(10 ** (-6), 10 ** (-1), num_iter)
Ms, lrs

hyperparameter_list = sorted(hyperparameter_list, key=lambda x: x[0])
M1, lr1 = hyperparameter_list[0][2:0:-1]
M2, lr2 = hyperparameter_list[1][2:0:-1]
M3, lr3 = hyperparameter_list[2][2:0:-1]

model1 = a_model(M1)
optimizer1 = SGD(model1.parameters(), lr1)

model2 = a_model(M2)
optimizer2 = SGD(model2.parameters(), lr2)

model3 = a_model(M3)
optimizer3 = SGD(model3.parameters(), lr3)

# We are only using 50 epochs for this example. You may have to use more.
epoch_num = 40
train_loss1, train_accuracy1, val_loss1, val_accuracy1 = train(
    model1, optimizer1, train_loader, val_loader, epoch_num
)

train_loss2, train_accuracy2, val_loss2, val_accuracy2 = train(
    model2, optimizer2, train_loader, val_loader, epoch_num
)

train_loss3, train_accuracy3, val_loss3, val_accuracy3 = train(
    model3, optimizer3, train_loader, val_loader, epoch_num
)

"""Plot the training and validation accuracy for each epoch."""

epochs = range(1, epoch_num+1)
plt.plot(epochs, train_accuracy1, label="Train Accuracy 1")
plt.plot(epochs, val_accuracy1, ':', label="Validation Accuracy 1")
plt.plot(epochs, train_accuracy2, label="Train Accuracy 2")
plt.plot(epochs, val_accuracy2, ':', label="Validation Accuracy 2")
plt.plot(epochs, train_accuracy3, label="Train Accuracy 3")
plt.plot(epochs, val_accuracy3, ':', label="Validation Accuracy 3")
plt.axhline(y=0.5, color='r', linestyle='-')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Part A Accuracy for CIFAR-10 vs Epoch")
plt.show()

"""The last thing we have to do is evaluate our model on the testing data."""

def evaluate(
    model: nn.Module, loader: DataLoader
) -> Tuple[float, float]:
    """Computes test loss and accuracy of model on loader."""
    loss = nn.CrossEntropyLoss()
    model.eval()
    test_loss = 0.0
    test_acc = 0.0
    with torch.no_grad():
        for (batch, labels) in loader:
            batch, labels = batch.to(DEVICE), labels.to(DEVICE)
            y_batch_pred = model(batch)
            batch_loss = loss(y_batch_pred, labels)
            test_loss = test_loss + batch_loss.item()

            pred_max = torch.argmax(y_batch_pred, 1)
            batch_acc = torch.sum(pred_max == labels)
            test_acc = test_acc + batch_acc.item()
        test_loss = test_loss / len(loader)
        test_acc = test_acc / (batch_size * len(loader))
        return test_loss, test_acc

test_loss, test_acc = evaluate(model1, test_loader)
print(f"Test Accuracy: {test_acc}")

assert torch.cuda.is_available(), "GPU is not available, check the directions above (or disable this assertion to use CPU)"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)  # this should print out CUDA

def b_model() -> nn.Module:

    model =  nn.Sequential(

          nn.Conv2d(in_channels=3, out_channels=100, kernel_size=5),
          nn.ReLU(),
          nn.MaxPool2d(14),
          nn.Flatten(),
          nn.Linear(400, 10)
         )
    return model.to(DEVICE)

def parameter_search_b(
    train_loader: DataLoader,
    val_loader: DataLoader,
    model_fn:Callable[[], nn.Module]
) -> float:
    """
    Parameter search for our linear model using SGD.

    Args:
    train_loader: the train dataloader.
    val_loader: the validation dataloader.
    model_fn: a function that, when called, returns a torch.nn.Module.

    Returns:
    The learning rate with the least validation loss.
    NOTE: you may need to modify this function to search over and return
     other parameters beyond learning rate.
    """
    num_iter = 20
    best_loss = torch.tensor(np.inf)
    best_lr = 0.0
    best_mom = 0.0

    lrs = torch.linspace(10 ** (-2), 0.125, num_iter)
    momentums  = torch.linspace(0.01, 0.99, int(num_iter/2))
    hyperparameter_list = []

    for mom in momentums:
      for lr in lrs:
        print(f"trying learning rate {lr}", f"trying momentum {mom}")
        model = b_model()
        optim = SGD(model.parameters(), lr, momentum = mom)

        train_loss, train_acc, val_loss, val_acc = train(
            model,
            optim,
            train_loader,
            val_loader,
            epochs=10
            )

        if min(val_loss) < best_loss:
            best_loss = min(val_loss)
            best_lr = lr
            best_mom = mom
            #print(type(best_lr))
            hyperparameter_list.append((best_loss, best_mom, best_lr))

    return best_lr, best_mom, hyperparameter_list

best_lr, best_mom, hyperparameter_list = parameter_search_b(train_small_loader, val_small_loader, b_model)

hyperparameter_list = sorted(hyperparameter_list, key=lambda x: x[0])
M1, lr1 = hyperparameter_list[0][1:]
M2, lr2 = hyperparameter_list[1][1:]
M3, lr3 = hyperparameter_list[2][1:]

hyperparameter_list
# For record
[(1.503855761885643, tensor(0.8811), tensor(0.1250)),
 (1.5121110260486603, tensor(0.7722), tensor(0.1189)),
 (1.5420830637216567, tensor(0.7722), tensor(0.1068)),
 (1.567347963154316, tensor(0.6633), tensor(0.1189)),
 (1.612675067782402, tensor(0.6633), tensor(0.1129)),
 (1.6223463773727418, tensor(0.6633), tensor(0.1068)),
 (1.628960946202278, tensor(0.5544), tensor(0.1129)),
 (1.6603348284959794, tensor(0.5544), tensor(0.1068)),
 (1.6606923788785934, tensor(0.4456), tensor(0.1129)),
 (1.6898588478565215, tensor(0.4456), tensor(0.0887)),
 (1.6902833253145217, tensor(0.3367), tensor(0.1189)),
 (1.7124766200780868, tensor(0.3367), tensor(0.1008)),
 (1.712677538394928, tensor(0.2278), tensor(0.1250)),
 (1.7288034766912461, tensor(0.2278), tensor(0.1129)),
 (1.7505502194166183, tensor(0.1189), tensor(0.1129)),
 (1.7690792113542557, tensor(0.1189), tensor(0.0947)),
 (1.7734861761331557, tensor(0.0100), tensor(0.1129)),
 (1.8090637177228928, tensor(0.0100), tensor(0.0947)),
 (1.8276431262493134, tensor(0.0100), tensor(0.0826)),
 (1.844857633113861, tensor(0.0100), tensor(0.0766)),
 (1.8772615134716033, tensor(0.0100), tensor(0.0705)),
 (1.9133204400539399, tensor(0.0100), tensor(0.0645)),
 (1.9212776452302933, tensor(0.0100), tensor(0.0584)),
 (1.9761168509721756, tensor(0.0100), tensor(0.0524)),
 (1.9799859762191772, tensor(0.0100), tensor(0.0463)),
 (2.029427480697632, tensor(0.0100), tensor(0.0403)),
 (2.0501161098480223, tensor(0.0100), tensor(0.0342)),
 (2.0899963438510896, tensor(0.0100), tensor(0.0282)),
 (2.14000244140625, tensor(0.0100), tensor(0.0221)),
 (2.203778403997421, tensor(0.0100), tensor(0.0161)),
 (2.2305036962032316, tensor(0.0100), tensor(0.0100))]

bmodel = b_model()
optimizer1 = SGD(bmodel.parameters(), lr1, momentum = M1)
optimizer2 = SGD(bmodel.parameters(), lr2, momentum = M2)
optimizer3 = SGD(bmodel.parameters(), lr3, momentum = M3)

# We are only using 70 epochs for this example. You may have to use more.
epoch_num = 70
train_loss1, train_accuracy1, val_loss1, val_accuracy1 = train(
    bmodel, optimizer1, train_loader, val_loader, epoch_num
)

train_loss2, train_accuracy2, val_loss2, val_accuracy2 = train(
    bmodel, optimizer2, train_loader, val_loader, epoch_num
)

train_loss3, train_accuracy3, val_loss3, val_accuracy3 = train(
    bmodel, optimizer3, train_loader, val_loader, epoch_num
)

epochs = range(1, epoch_num+1)
plt.plot(epochs, train_accuracy1, label="Train Accuracy 1")
plt.plot(epochs, val_accuracy1, ':', label="Validation Accuracy 1")
plt.plot(epochs, train_accuracy2, label="Train Accuracy 2")
plt.plot(epochs, val_accuracy2, ':', label="Validation Accuracy 2")
plt.plot(epochs, train_accuracy3, label="Train Accuracy 3")
plt.plot(epochs, val_accuracy3, ':', label="Validation Accuracy 3")
plt.axhline(y=0.65, color='r', linestyle='-')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Part B Accuracy for CIFAR-10 vs Epoch")
plt.show()

test_loss, test_acc = evaluate(bmodel, test_loader)
print(f"Test Accuracy: {test_acc}")

