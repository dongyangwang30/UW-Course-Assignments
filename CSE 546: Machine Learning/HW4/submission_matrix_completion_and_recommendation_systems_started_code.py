# -*- coding: utf-8 -*-
"""Submission: matrix-completion-and-recommendation-systems-started-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m-ozkdHlWXw0rr5c7lNBCsl1gxORwj63

# Matrix Completion and Recommendation System

In this problem, we will use the 100K MovieLens dataset available at https://grouplens.org/datasets/movielens/100k/ to estimate unknown user ratings given their previous ratings.

Create a copy of this notebook on your personal drive as started code. Download the dataset and upload ``u.data`` under the "Files" tab in the sidebar (**NOTE:** you will need to re-upload this file if you disconnect and delete the current runtime).
"""

import csv
import numpy as np
from scipy.sparse.linalg import svds
import matplotlib.pyplot as plt
import torch

"""Let's load the 100K MovieLens data."""

data = []
with open('u.data') as csvfile:
    spamreader = csv.reader(csvfile, delimiter='\t')
    for row in spamreader:
        data.append([int(row[0])-1, int(row[1])-1, int(row[2])])
data = np.array(data)

num_observations = len(data)  # num_observations = 100,000
num_users = max(data[:,0])+1  # num_users = 943, indexed 0,...,942
num_items = max(data[:,1])+1  # num_items = 1682 indexed 0,...,1681

np.random.seed(1)
num_train = int(0.8*num_observations)
perm = np.random.permutation(data.shape[0])
train = data[perm[0:num_train],:]
test = data[perm[num_train::],:]

print(f"Successfully loaded 100K MovieLens dataset with",
      f"{len(train)} training samples and {len(test)} test samples")

"""## Part a
Our first estimator pools all users together and, for each movie, outputs as its prediction the average user rating of that movie in ``train``. That is, if $\mu \in \mathbb{R}^m$ is a vector where $\mu_i$ is the average rating of the users that rated the $i$-th movie. Write this estimator $\widehat{R}$ as a rank-one matrix.

Compute the estimate $\widehat{R}$. What is $\mathcal{E}_{\rm test} (\widehat{R})$ for this estimate?
"""

train.shape

# Your code goes here
# Compute estimate

import timeit

start = timeit.default_timer()

#Your statements here


avg_rating = {}
for i in train:
  if i[1] in avg_rating:
    avg_rating[i[1]].append(i[2])
  else:
    avg_rating[i[1]] = [i[2]]
for i in avg_rating.keys():
  avg_rating[i] = np.mean(avg_rating[i])

test_pred = test.copy()
test_pred = test_pred.tolist()

for i in range(len(test_pred)):
  if test_pred[i][1] in avg_rating:
    test_pred[i][2] = float(avg_rating[test_pred[i][1]])
  else:
    test_pred[i][2] = np.mean(list(avg_rating.values()))

true = test[:,2]
pred = np.array([k for i,j,k in test_pred])

test_error = np.mean((true-pred)**2)
stop = timeit.default_timer()

print('Time: ', stop - start)

start = timeit.default_timer()

R = np.ones((1682,1))
# Compute estimate
for i in np.unique(train[:,1]):
    R[i]=np.mean(train[np.where(train[:,1]==i),2])

# Evaluate test error
test_error= np.mean((test[:,[2]]- R[test[:,1]])**2)
test_error

stop = timeit.default_timer()

print('Time: ', stop - start)

# Evaluate test error

test_error

"""## Part b
Create a matrix $\widetilde{R}_{i, j} \in \mathbb{R}^{m \times n}$ and set its entries equal to the known values in the training set, and $0$ otherwise.

Let $\widehat{R}^{(d)}$ be the best rank-$d$ approximation (in terms of squared error) approximation to $\widetilde{R}$. This is equivalent to computing the singular value decomposition (SVD) and using the top $d$ singular values. This learns a lower-dimensional vector representation for users and movies, assuming that each user would give a rating of $0$ to any movie they have not reviewed.

- For each $d = 1, 2, 5, 10, 20, 50$, compute the estimator $\widehat{R}^{(d)}$. We recommend using an efficient solver, such as ``scipy.sparse.linalg.svds``.
- Plot the average squared error of predictions on the training set and test set on a single plot, as a function of $d$.
"""

# Create the matrix R twiddle (\widetilde{R}).
r_twiddle = np.zeros([num_items, num_users])
for i in train:
  r_twiddle[i[1], i[0]] = i[2]

# Your code goes here
def construct_estimator(d, r_twiddle):
  u,s,v = svds(r_twiddle, k =d)
  u_d = u[:, :d]
  s_d = np.diag(s[:d])
  v_d = v[:d, :]
  return u_d @ s_d @ v_d

def get_error(d, r_twiddle, dataset):
  r_data = np.zeros([num_items, num_users])
  for i in dataset:
    r_data[i[1], i[0]] = i[2]
  pred = construct_estimator(d, r_twiddle)
  return np.mean((pred - r_data)**2)

# Your code goes here
# Evaluate train and test error for: d = 1, 2, 5, 10, 20, 50.
list_d = [1, 2, 5, 10, 20, 50]
train_error = []
test_error = []

for d in list_d:
  train_error.append(get_error(d, r_twiddle, train))
  test_error.append(get_error(d, r_twiddle, test))

# Plot both train and test error as a function of d on the same plot.

plt.plot(list_d, train_error, label='Train Error')
plt.plot(list_d, test_error, label='Test Error')
plt.xlabel('Rank (d)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('Train and Test Error vs. Rank (d)')
plt.show()

print(train_error, test_error )

"""## Part c
Replacing all missing values by a constant may impose strong and potentially incorrect assumptions on the unobserved entries of $R$. A more reasonable choice is to minimize the mean squared error (MSE) only on rated movies. Define a loss function:
$$
\mathcal{L} \left( \{u_i\}_{i=1}^m, \{v_j\}_{j=1}^n \right) :=
\sum_{(i, j, R_{i, j}) \in {\rm train}} (\langle u_i,v_j\rangle - R_{i,j})^2 +
\lambda \sum_{i=1}^m \|u_i\|_2^2 +
\lambda \sum_{j=1}^n \|v_j\|_2^2
$$
where $\lambda > 0$ is the regularization coefficient. We will implement algorithms to learn vector representations by minimizing the above loss. You may need to tune $\lambda$ and $\sigma$ to optimize the loss.

Implement alternating minimization (as defined in the homework spec) and plot the MSE of ``train`` and ``test`` for $d \in \{1, 2, 5, 10, 20, 50\}$.
"""

# Your code goes here
def closed_form_u(V, U, l):

  A = V.T @ V + l * np.eye(V.shape[1])
  B = r_twiddle @ V
  U_new = np.linalg.solve(A,B.T)
  return U_new.T

def closed_form_v(V, U, l):
  A = U.T @ U + l * np.eye(U.shape[1])
  B = r_twiddle.T @ U
  #print(r_twiddle.shape, A.shape, B.shape)
  V_new = np.linalg.solve(A, B.T)
  #print(V_new.T.shape)
  return V_new.T

def construct_alternating_estimator(
    d, r_twiddle, l=0.0, delta=1e-5, sigma=0.1, U=None, V=None
):
    max_iter = 0
    while True:
      if max_iter >10000:
        break
      U_new = closed_form_u(V, U, l)
      V_new = closed_form_v(V, U_new, l)

      diff_U = np.linalg.norm(U_new - U)
      diff_V = np.linalg.norm(V_new - V)
      if diff_U < delta and diff_V < delta:
        break

      U = U_new
      V = V_new
      max_iter += 1
      # print("iteration", max_iter)
    return U_new, V_new

np.random.seed(42)
n, m = r_twiddle.shape

def get_error_1(d, r_twiddle, dataset, U_final, V_final):
  r_data = np.zeros([num_items, num_users])
  for i in dataset:
    r_data[i[1], i[0]] = i[2]
  pred = U_final @ V_final.T
  return np.mean((pred - r_data)**2)


# Evaluate train and test error for: d = 1, 2, 5, 10, 20, 50.
list_d = [1, 2, 5, 10, 20, 50]
train_error = []
test_error = []

for d in list_d:
  U = np.random.randn(n, d)
  V = np.random.randn(m, d)
  U_final, V_final = construct_alternating_estimator(d, r_twiddle, l=20.0, delta=1e-5, sigma=0.1, U=U, V=V)
  train_error.append(get_error_1(d, r_twiddle, train, U_final, V_final))
  test_error.append(get_error_1(d, r_twiddle, test, U_final, V_final))

print(train_error, test_error)

# Plot both train and test error as a function of d on the same plot.

plt.plot(list_d, train_error, label='Train Error')
plt.plot(list_d, test_error, label='Test Error')
plt.xlabel('Rank (d)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('Train and Test Error vs. Rank (d)')
plt.show()

"""## Part d (Extra credit)
Implement any algorithm you'd like (you must implement it yourself; do not use an off-the-shelf algorithm from e.g. ``scikit-learn``) to find an estimator that achieves a test error of less than 0.9.

**NOTE:** This is extra credit. Please do not start unless you have finished all other parts of this homework!
"""

U_final, V_final = construct_alternating_estimator(d, r_twiddle, l=100.0, delta=1e-5, sigma=0.1, U=U, V=V)
get_error_1(d, r_twiddle, test, U_final, V_final)

"""Scratch Paper below"""

A = V.T @ V + l * np.eye(V.shape[1])
B = r_twiddle @ V
U_new = np.linalg.solve(A,B.T)

A = V.T @ V + 10 * np.eye(V.shape[1])
B = r_twiddle @ V
U_new1 = np.linalg.solve(A,B.T)

U_new1

U_new

train_error

r_data = np.zeros([num_items, num_users])
for i in train:
  r_data[i[1], i[0]] = i[2]
pred = U_final @ np.eye(d) @ V_final.T

print(U_final.shape, np.eye(1).shape, V_final.shape)

d

get_error_1(d, r_twiddle, test)

print(U_final.shape, (np.diag(s[:d])).shape, V_final.shape)

np.eye(d).shape

l = 0.1
def closed_form_v(V, U, l):
  A = U.T @ U + l * np.eye(U.shape[1])
  B = r_twiddle.T @ U
  print(r_twiddle.shape, A.shape, B.shape)
  V_new = np.linalg.solve(A, B.T)
  print(V_new.T.shape)
  return V_new.T

#A = U.T @ U + 0.1 * np.eye(U.shape[1])
##B = r_twiddle.T @ U
#print(A.shape, B.shape)
V_new = closed_form_v(V, U, l)

def closed_form_u(V, U, l):

  A = V.T @ V + l * np.eye(V.shape[1])
  B = r_twiddle @ V
  print(r_twiddle.shape, A.shape, B.shape)
  U_new = np.linalg.solve(A,B.T)
  return U_new


#print(r_twiddle.shape, U.shape, V.shape)
#print(r_twiddle @ V)
A = V.T @ V + 0.1 * np.eye(V.shape[1])
B = r_twiddle @ V
#print(A.shape, B.T)
#U_new = np.linalg.solve(A,B.T)
U_new = closed_form_u(V, U, l)
print(U_new.shape)

print(r_twiddle.shape, U.shape, V.shape)

l = 1
np.linalg.inv(V @ V.T + l * np.eye(V.shape[0])).shape

A = V.T @ V + l * np.eye(V.shape[1])
B = V.T @ r_twiddle.T
U_new = np.linalg.solve(A, B)

(r_twiddle@ V).shape

V.shape

