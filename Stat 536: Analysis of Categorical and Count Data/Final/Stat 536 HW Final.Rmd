---
title: "Stat 536 Final"
author: "Dongyang Wang"
date: "2022-12-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

### 1

```{r}
rm(list = ls())
df <- c(1105, 4624, 411111, 157342, 14, 497, 483, 1008)
df.array <- array(df, c(2,2,2))
df.array

saturated.loglin = loglin(df.array,margin=list(c(1,2,3)),
                                       fit=TRUE,param=TRUE)

indep.loglin = loglin(df.array,margin=list(1,2,3),
                                 fit=TRUE,param=TRUE)

X1indepX2X3.loglin = loglin(df.array ,margin=list(1,c(2,3)),
                                          fit=TRUE,param=TRUE)

X2indepX1X3.loglin = loglin(df.array ,margin=list(2,c(1,3)),
                                              fit=TRUE,param=TRUE)
                            
X3indepX1X2.loglin = loglin(df.array ,margin=list(3,c(1,2)),
                                       fit=TRUE,param=TRUE)    

X2indepX3givenX1.loglin = loglin(df.array ,margin=list(c(1,2),c(1,3)),
                                fit=TRUE,param=TRUE)

X1indepX3givenX2.loglin = loglin(df.array ,margin=list(c(1,2),c(2,3)),
                                  fit=TRUE,param=TRUE)

X1indepX2givenX3.loglin = loglin(df.array ,margin=list(c(1,3),c(2,3)),
                      fit=TRUE,param=TRUE)

no2nd.loglin = loglin(df.array ,margin=list(c(1,2),c(1,3),c(2,3)),
                                       fit=TRUE,param=TRUE)
```

Then we assess their fit.
```{r}
1-pchisq(indep.loglin$lrt,indep.loglin$df)
1-pchisq(X1indepX2X3.loglin$lrt,X1indepX2X3.loglin$df)
1-pchisq(X2indepX1X3.loglin$lrt,X2indepX1X3.loglin$df)
1-pchisq(X3indepX1X2.loglin$lrt,X3indepX1X2.loglin$df)
1-pchisq(X2indepX3givenX1.loglin$lrt,X2indepX3givenX1.loglin$df)
1-pchisq(X1indepX3givenX2.loglin$lrt,X1indepX3givenX2.loglin$df)
1-pchisq(X1indepX2givenX3.loglin$lrt,X1indepX2givenX3.loglin$df)
1-pchisq(no2nd.loglin$lrt,no2nd.loglin$df)
```
Based on above results, the no second order interaction model fits data well (fail to reject the null hypothesis that the model does not fit the data). Details about the model below:

```{r}
no2nd.loglin
```

An expression for this model is $logm_{ijk} = u + u_{1(i)} + u_{2(j)} + u_{3(k)} + u_{12(ij)} + u_{13(ik)} +u_{23(jk)}$.

### 2

$$\frac{P(X_3=2|X_1=i, X_2=j)}{P(X_3=1|X_1=i, X_2=j)}=exp((\hat{u}_{3(2)} - \hat{u}_{3(1)}) +(\hat{u}_{13(i2)} - \hat{u}_{13(i1)}) +  (\hat{u}_{23(j2)} - \hat{u}_{23(j1)}))$$

which is equivalent to 

For i = 1, j = 1

```{r}
exp((-2.251693 - ( 2.251693)) + ( -0.4293324 - 0.4293324) + ( 0.6994481 - (- 0.6994481))  )
```

For i = 1, j = 2

```{r}
exp(( -2.251693 - (2.251693)) + (-0.4293324 - 0.4293324) + (- 0.6994481 -0.6994481 )  )
```

For i = 2, j = 1

```{r}
exp((-2.251693 - (+ 2.251693)) + (0.4293324 - (-0.4293324)) + (0.6994481 - (-0.6994481))  )
```

For i = 2, j = 2

```{r}
exp((-2.251693 - (2.251693)) + (0.4293324 - (-0.4293324)) + ( -0.6994481 - 0.6994481)  )
```

Based on the above results, we learn that the odds of fatality is small regardless of the conditions.

### 3

In the following reconstruction of the dataset, x1 is seat belt and is 1 if yes; x2 is ejection and is 1 if yes; x3 is injury and 2 if fatal.

```{r}
mydata = matrix(c(rep(c(1,1,1),1105),rep(c(2,1,1),4624),rep(c(1,2,1),411111),
                  rep(c(2,2,1),157342),rep(c(1,1,2),14),rep(c(2,1,2),497),
                  rep(c(1,2,2),483),rep(c(2,2,2),1008)),ncol = 3, byrow = TRUE)

mylogit = glm(factor(mydata[,3])~1, 
              family=binomial(link=logit))
mylogit_1 = glm(factor(mydata[,3])~factor(mydata[,1]), 
                 family=binomial(link=logit))
mylogit_2= glm(factor(mydata[,3])~factor(mydata[,2]), 
                family=binomial(link=logit))
mylogit_all= glm(factor(mydata[,3])~factor(mydata[,1])+factor(mydata[,2]),
                 family=binomial(link=logit))

AIC(mylogit)
AIC(mylogit_1)
AIC(mylogit_2)
AIC(mylogit_all)

```

Based on the AIC, we select the last comprehensive logistic regression model.

```{r}
summary(mylogit_all)
```

### 4

Based on the results from above three sections, we know that seat belts and whether they are ejected both matter in terms of whether car injuries are fatal. Furthermore, There is a negative correlation between ejection and seat belt, with the correlation being -0.1394401 as shown below. Based on the coefficient from part 3, whether a person is ejected from the car is more relevant for fatality.

```{r}
cor(mydata[,2], mydata[,3])
```

## Question 2

```{r}
rm(list = ls())
library(Rgraphviz)
library(gRbase)
library(graph)
library(gRim)
```

```{r}
# data set
#data(gRbase::reinis)
getdata <- function(...)
{
    e <- new.env()
    name <- data(..., envir = e)[1]
    e[[name]]
}
reinis1 = getdata(reinis)
str(reinis1)
```
To obtain the relevant log linear models, one approach is to replicate the paper discussed in class, namely starting from the saturated model and deleting edges and adding edges by searches. However, a more convenient approach is use the stepwise approach and calculate the model that achieves the lowest AIC or BIC. Therefore, we only need one relevant model, that is the saturated model as listed below.

```{r}
# saturated model
m<-dmod(~.^.,data=reinis1)
m
```

We then do the model selection by AIC and BIC, and visualize the results.

```{r}
# AIC for model selection
step_m <- stepwise(m)
step_m

# BIC for model selection
step_m2 <- stepwise(m,k=log(sum(reinis1)))
step_m2

# visualize the models
plot(step_m)
plot.new()
plot(step_m2)
```

We can, for example, choose the model with lowest BIC. The formula is listed below.

```{r}
formula(step_m2)
as(step_m2, "matrix")
```

We also attempt the Graph estimation in GGMs using the birth-death MCMC.

```{r}
library(BDgraph)
reinis2 = getdata(reinis)
sample <- bdgraph.mpl(data=reinis2, method = "dgm-binary",
                      algorithm = "bdmcmc", iter = 10000,
                      burnin = 6000)
select(sample, cut=0.5,vis = TRUE)
```

Therefore, based on model selection of BIC and Graph estimation in GGMs using the birth-death MCMC we obtain the result that family is not related with other variables. For the logistic regression, we would need include any variable of ABCDE, and only include the constant term based on our results above.