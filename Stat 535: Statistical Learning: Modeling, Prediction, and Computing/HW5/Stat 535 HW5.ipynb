{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32e9fb4",
   "metadata": {},
   "source": [
    "# Stat 535 HW5\n",
    "Dongyang Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f09d34",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903223d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://sites.stat.washington.edu/mmp/courses/535/fall22/assignments/hw4-nn-train-100.dat')\n",
    "hw4_nn = response.text\n",
    "\n",
    "data_by_line = hw4_nn.split('\\n')\n",
    "data_by_line = data_by_line[:len(data_by_line)-1]\n",
    "\n",
    "for i in range(0,len(data_by_line)):\n",
    "    data_by_line[i] = ' '.join(data_by_line[i].split())\n",
    "    data_by_line[i] = data_by_line[i].split(' ')\n",
    "\n",
    "hw4_nn = [0] * len(data_by_line)\n",
    "    \n",
    "for i in range(len(data_by_line)):\n",
    "    hw4_nn[i] = [float(j) for j in data_by_line[i]]\n",
    "\n",
    "nn_train = pd.DataFrame(hw4_nn, columns =['x1', 'x2', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://sites.stat.washington.edu/mmp/courses/535/fall22/assignments/hw4-nn-test.dat')\n",
    "hw4_nn = response.text\n",
    "\n",
    "data_by_line = hw4_nn.split('\\n')\n",
    "data_by_line = data_by_line[:len(data_by_line)-1]\n",
    "\n",
    "for i in range(0,len(data_by_line)):\n",
    "    data_by_line[i] = ' '.join(data_by_line[i].split())\n",
    "    data_by_line[i] = data_by_line[i].split(' ')\n",
    "\n",
    "hw4_nn = [0] * len(data_by_line)\n",
    "    \n",
    "for i in range(len(data_by_line)):\n",
    "    hw4_nn[i] = [float(j) for j in data_by_line[i]]\n",
    "\n",
    "nn_test = pd.DataFrame(hw4_nn, columns =['x1', 'x2', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6afc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(nn_train['x1'], nn_train['x2'], c = nn_train['y'])\n",
    "\n",
    "plt.title(\"Neural Network by Class (Yellow for Positive)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c5ebb",
   "metadata": {},
   "source": [
    "### b\n",
    "\n",
    "No, it is impossible to do that because the decision boundary is irregular and we need way more than 2 units to perfectly seperate the +1 and -1 cases. So we will need more than 2 units in the layer to get perfect classification where $\\hat{L}_{01} = 0$.\n",
    "\n",
    "### c\n",
    "\n",
    "I have chosen the initial points from a uniform distribution with a small interval, namely [-0.5,0.5]. This way, I randomly generate my initial points and not to get too extreme values for the sigmoid functions.\n",
    "\n",
    "Note that I have ignored the tolerance becuase the change in the beta and w values are very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80279d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "#if nn_train['y'][i] == 1:\n",
    "#        Y.append([nn_train['y'][i]])\n",
    "#    else:\n",
    "#        Y.append([0])\n",
    "for i in range(len(nn_train)):\n",
    "    Y.append([nn_train['y'][i]])\n",
    "    X.append([1, nn_train['x1'][i], nn_train['x2'][i]])\n",
    "\n",
    "Y = np.matrix(Y)\n",
    "X = np.matrix(X)\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(len(nn_test)):\n",
    "    Y_test.append([nn_test['y'][i]])\n",
    "    X_test.append([1, nn_test['x1'][i], nn_test['x2'][i]])\n",
    "Y_test = np.matrix(Y_test)\n",
    "X_test = np.matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef45344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_beta(y, yhat):\n",
    "    sub = np.subtract(y, yhat)\n",
    "    return sub.mean()\n",
    "\n",
    "def get_gradient_w(y, yhat, x):\n",
    "    y_yhat = np.subtract(y, yhat)\n",
    "    res = np.matmul(y_yhat, x)\n",
    "    return res.mean()\n",
    "\n",
    "def prediction(b, w, x, b0):\n",
    "    xt = x.transpose()\n",
    "    wxt = np.matmul(w, xt)\n",
    "    z = 1/(1+np.exp(wxt))\n",
    "    bt = b.transpose()\n",
    "    val = np.matmul(bt, z)[0,0]+b0\n",
    "    return  1/(1+np.exp(val))\n",
    "\n",
    "#def get_logit(y, w, x):\n",
    "#    exponent = -1 * y[0,0] * prediction(b, w, x, b0)\n",
    "#    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "def get_logit_one_w(y, wi, x):\n",
    "    xt = x.transpose()\n",
    "    wxt = np.matmul(wi, xt)[0,0]\n",
    "    exponent = -1*(y[0,0]*wxt)\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "def get_logit(y, w, x):\n",
    "    logit = 0\n",
    "    for wi in w:\n",
    "        logit += abs(get_logit_one_w(y, wi, x))\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579be8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575db2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43eec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e1987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bc084f0",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "#### a\n",
    "\n",
    "Based on definition, $J_{\\lambda1} = \\hat{L}(w_1) + \\frac{\\lambda_1}{2} ||w_1||^2 < \\hat{L}(w_2) + \\frac{\\lambda_1}{2} ||w_2||^2$, similarly, $J_{\\lambda2} = \\hat{L}(w_2) + \\frac{\\lambda_2}{2} ||w_2||^2 < \\hat{L}(w_1) + \\frac{\\lambda_2}{2} ||w_1||^2$. Since \n",
    "$\\lambda_1 > \\lambda_2$, $\\hat{L}(w_2) + \\frac{\\lambda_2}{2} ||w_2||^2 < \\hat{L}(w_1) + \\frac{\\lambda_2}{2} ||w_1||^2 < \\hat{L}(w_1) + \\frac{\\lambda_1}{2} ||w_1||^2 < \\hat{L}(w_2) + \\frac{\\lambda_1}{2} ||w_2||^2$. Assume, to the contrary, $||w_1|| > ||w_2||$, $\\hat{L}(w_2) + \\frac{\\lambda_2}{2} ||w_2||^2 < \\hat{L}(w_2) + \\frac{\\lambda_1}{2} ||w_2||^2 < \\hat{L}(w_2) + \\frac{\\lambda_1}{2} ||w_1||^2$ and therefore $\\frac{\\lambda_2}{2} ||w_2||^2 > \\frac{\\lambda_1}{2} ||w_1||^2$. Thus, $\\frac{||w_1||^2}{||w_2||^2} < \\frac{\\lambda_2}{\\lambda_1}$, which is a contradiction. Therefore, $||w_1|| < ||w_2||$.\n",
    "\n",
    "In ridge regression, for example,\n",
    "\n",
    "$\\hat{w} = (X^TX + \\lambda I_n)^{-1}X^TY$. We can observe that with an increase in $\\lambda$, The inverse of $X^TX + \\lambda I_n$ decreases, therefore regularized quantity dedcreases as more regularization is imposed.\n",
    "\n",
    "#### b\n",
    "\n",
    "Since $\\lambda_1 > \\lambda_2$, and proved above, $||w_2|| > ||w_1||$. Given $\\hat{L}(w_2) + \\frac{\\lambda_2}{2} ||w_2||^2 < \\hat{L}(w_1) + \\frac{\\lambda_2}{2} ||w_1||^2$, $\\hat{L}(w_2) - \\hat{L}(w_1) < \\frac{\\lambda_2}{2} ||w_1||^2 - \\frac{\\lambda_2}{2} ||w_2||^2<0$ and therefore $\\hat{L}(w_2) < \\hat{L}(w_1)$.\n",
    "\n",
    "In ridge regression, for example,\n",
    "\n",
    "$\\hat{L} = \\sum (y - w^T x)^2$ and with an increase in regularization lambda there is decrease in regularized quantity w, such that $y - w^T x$ increases and so is $\\hat{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df541cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
